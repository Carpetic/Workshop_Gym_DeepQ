{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b949a63",
   "metadata": {},
   "source": [
    "# Workshop Open Ai Gym and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d96a286",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym==0.21.0\n",
    "%pip install numpy\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7103c06",
   "metadata": {},
   "source": [
    "## Environement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7204cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ca477",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "We will be using the OpenAI [Gymnasium](https://gymnasium.farama.org/)(gym) library to create our AI algorithm. \n",
    "This library offers the opportunity to choose from a variety of environments\n",
    "\n",
    "we will be working with the [MountainCar](https://gymnasium.farama.org/environments/classic_control/mountain_car/) environment. \n",
    "To get started, you will need to initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b037ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "# Create the env here\n",
    "Qpts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f0270",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26721f70",
   "metadata": {},
   "source": [
    "Now that the environment has been created, we need to write the functions to train our AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865436c",
   "metadata": {},
   "source": [
    "We use this function to calculate the reward based on the car's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6922dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_calcul(pos):\n",
    "    if (pos >= 0.5):\n",
    "        return 2\n",
    "    else:\n",
    "        return (pos + 1.2) / 1.8 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd6692",
   "metadata": {},
   "source": [
    "### Â The stateSpace func returns the size of the state space as an numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b91dff",
   "metadata": {},
   "source": [
    "First, subtract the lower bound of the observation space from the upper bound. Then, scale the result using a numpy array of [10, 50].\n",
    "\n",
    "Finally, round the result to the nearest integer, cast it as an int, and add 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf21fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateSpace(env):\n",
    "    # Calculate the state space here\n",
    "    return size_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1126752",
   "metadata": {},
   "source": [
    "### The lower_state func returns the normalized and rounded state of the environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18b7af",
   "metadata": {},
   "source": [
    "First, call the **'reset'** method on the environment to initialize its state.\n",
    "\n",
    "Next, normalize the state of the environment by subtracting the lower bound of the observation space (observation_space.low) and scaling it using a numpy array of [10, 50].\n",
    "\n",
    "Finally, round the result to the nearest integer and cast it as an int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa8ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_state(env):\n",
    "    # Get the state and calculate the lower state here\n",
    "    return low_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f239dd",
   "metadata": {},
   "source": [
    "### The update func return a delta to update the values in a Q-table using a Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70acdb",
   "metadata": {},
   "source": [
    "To calculate the *delta* value, follow these steps:\n",
    "\n",
    "1. Compute the target Q-value by using the *reward* of the *next state* and the *maximum Q-value* over all actions in the *adjusted next state*.\n",
    "\n",
    "2. Calculate the *delta* as the *difference* between the *target Q-value* and the *current Q-value* for the *current state-action pair*.\n",
    "\n",
    "3. The function should *return* the calculated *delta* value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd8b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(learningRate, nextState, nextState_adj, Q_Table, currentState, action):\n",
    "    # calculate the target Q-value here\n",
    "    # calculate the delta here\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06259f",
   "metadata": {},
   "source": [
    "### Training function\n",
    "\n",
    "Before we begin, we need to initialize some values. After this, we can start looping over the number of episodes.\n",
    "\n",
    "To calculate the *next action*, two conditions are considered:\n",
    "\n",
    "1. Compute the action by using numpy's *'argmax'* function on the *row* of the *Q-table for the current state*.\n",
    "\n",
    "2. If the first condition is not met, the action is calculated *randomly* between *0* and the number of actions in the environment (*env.action_space.n*).\n",
    "\n",
    "Next, calculate the new *adjusted state*, similar to the *'lower_states'* function, but using the *next state variable*.\n",
    "\n",
    "Set the *final/terminate* state to the reward variable, or update delta and add it to the current state.\n",
    "\n",
    "Finally, *update epsilon* if it is *greater* than the *minimum epsilon* by *multiplying/decreasing* it by *eps1*.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env, learningrate, discount, epsilon, min_epsilon, episodes):\n",
    "\n",
    "    reward_list = []\n",
    "    average_rewards = []    \n",
    "    eps1 = epsilon\n",
    "    first = episodes + 1\n",
    "    size_states = stateSpace(env)\n",
    "    \n",
    "    # initialisation of Q_table with random value\n",
    "    Q_Table = np.random.uniform(low = -1, high = 0, size = (size_states[0], size_states[1], env.action_space.n))    \n",
    "    Qinit = np.copy(Q_Table)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        tot_reward = 0\n",
    "        reward = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        state_adj = lower_state(env)\n",
    "        \n",
    "        while done != True:\n",
    "            # Calculate next action\n",
    "            # if np.random.random() < 1 - epsilon:\n",
    "            # numpy argmax on the row of Q_table on the current state\n",
    "            # else:\n",
    "            # random between 0 and the env.action_space.n\n",
    "                \n",
    "            # we get the info when we execute the action into the env\n",
    "            nextState, reward, done, info = env.step(action)\n",
    "            \n",
    "            # calculate nextState_adj here\n",
    "            \n",
    "            # if done and nextState[0] >= 0.5:\n",
    "                # set reward  as value for the current final state\n",
    "            # else:\n",
    "                # Update delta with the update function and add it to th current state in Q_table\n",
    "            \n",
    "            # Say when first success occurs\n",
    "            if nextState[0] >= 0.5 and i < first:\n",
    "                first = i\n",
    "                print('First time reaching goal on epsiode {}'.format(first + 1))\n",
    "            \n",
    "            tot_reward += rewards_calcul(nextState[0])\n",
    "            state_adj = nextState_adj\n",
    "        \n",
    "        # if epsilon is greater than min_epsilon, multiply/decreseate it by eps1\n",
    "            \n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            # Store the rewards list every 100 steps to calculate the average rewards \n",
    "            # print the average reward for this 100 steps\n",
    "            reward_list = []\n",
    "        \n",
    "    env.close()\n",
    "    # returns the average rewards, the final Q-Table and the initial Q-Table.\n",
    "    return average_rewards, Q_Table, Qinit\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8191605",
   "metadata": {},
   "source": [
    "### Now we can finnaly use our trainning function\n",
    "\n",
    "To ensure everything works, *reset the environment*.\n",
    "\n",
    "Next, call the *training* function with the following parameters: **env, 0.2, 0.9, 0.999, 0, and 10000**.\n",
    "\n",
    "Once the training is finished, *save the Q-points* (Q-table) into a *file*. You'll need to *reshape* the numpy array from *3-dimensional to 2-dimensional*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset env\n",
    "rewards, Qpts, Qinit = training(env, 0.2, 0.9, 0.999, 0, 10000)\n",
    "# save Qpts into file here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ddf60",
   "metadata": {},
   "source": [
    "### Plot of the Rewards\n",
    "\n",
    "Let's create a *plot* to visualize the *Average Reward per episode*.\n",
    "\n",
    "Use *Episodes* as the *x-axis label* and *Average Reward* as the *y-axis label*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aedbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(100 * (np.arange(len(rewards)) + 1), rewards)\n",
    "# set xlabel, ylabel, and title of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d3952",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e984b",
   "metadata": {},
   "source": [
    "Now that our AI has finished training, we can finally test it and see if it can complete the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ab724",
   "metadata": {},
   "source": [
    "### Load_data from a csv\n",
    "\n",
    "Let's write a function to *load* the data from the previously saved file so we can test our AI even after closing VSCode.\n",
    "\n",
    "Use the *numpy loadtxt* attribute to load the *CSV* and *reshape* the contents of the file with the *Q_table_shape*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b005c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Get file content\n",
    "    size_states = stateSpace(env)\n",
    "    Q_Table_shape = (size_states[0], size_states[1], env.action_space.n)\n",
    "    # Reshape of file content\n",
    "    return Qpts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d348dbf",
   "metadata": {},
   "source": [
    "### Testing function\n",
    "\n",
    "First, if the *Q_table* is *equal* to *None*, call the *load_data* function.\n",
    "\n",
    "In the loop, *render* the environment.\n",
    "\n",
    "Calculate the action by taking the *argmax* of the *Q-values* for the *current adjusted state* in the *Q-table*.\n",
    "\n",
    "Get the result of the step with this action, and finally, calculate the *adjusted state* like in the *lower_states* function but with the *next_state variable*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd8ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(env, Q_Table):\n",
    "    # load data if Q_table == None\n",
    "    state = env.reset()\n",
    "    state_adj = lower_state(env)\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        # Calculate the action by taking the argmax of the Q-values for the current adjusted state in the Q-table.\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # calculate the adjusted state like in the lower_states function but with the next_state variable.\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e0a79",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "To ensure that everything is working as expected, first call the reset method on the environment. Then, call the *testing* function.\n",
    "\n",
    "A window should open and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce13e7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'low_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQpts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m, in \u001b[0;36mtesting\u001b[0;34m(env, Q_Table)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtesting\u001b[39m(env, Q_Table):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# load data if Q_table == None\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m----> 4\u001b[0m     state_adj \u001b[38;5;241m=\u001b[39m \u001b[43mlower_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "Cell \u001b[0;32mIn [11], line 3\u001b[0m, in \u001b[0;36mlower_state\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower_state\u001b[39m(env):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Get the state and calculate the lower state here\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlow_state\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'low_state' is not defined"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "testing(env, Qpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad4203",
   "metadata": {},
   "source": [
    "## More ?\n",
    "\n",
    "Now you can attempt to optimize the algorithm by changing some parameters and observing the results. \n",
    "\n",
    "Alternatively, you can try a different algorithm or environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
